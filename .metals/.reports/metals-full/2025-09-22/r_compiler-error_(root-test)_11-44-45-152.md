error id: 40D9A034E18CC26E0E18176F01128A1B
file://<WORKSPACE>/src/test/scala/gibbon/performance/CheckpointPerformanceSpec.scala
### scala.reflect.internal.FatalError: 
  ThisType(value $anonfun) for sym which is not a class
     while compiling: file://<WORKSPACE>/src/test/scala/gibbon/performance/CheckpointPerformanceSpec.scala
        during phase: globalPhase=<no phase>, enteringPhase=parser
     library version: version 2.13.16
    compiler version: version 2.13.16
  reconstructed args: -classpath <WORKSPACE>/.bloop/root/bloop-bsp-clients-classes/test-classes-Metals-Ve0ZHR_bS9ClHmab5hD_vg==:<HOME>/Library/Caches/bloop/semanticdb/com.sourcegraph.semanticdb-javac.0.11.0/semanticdb-javac-0.11.0.jar:<WORKSPACE>/.bloop/root/bloop-bsp-clients-classes/classes-Metals-Ve0ZHR_bS9ClHmab5hD_vg==:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.13.16/scala-library-2.13.16.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-stream_2.13/2.8.5/akka-stream_2.13-2.8.5.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-http_2.13/10.5.3/akka-http_2.13-10.5.3.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-http-spray-json_2.13/10.5.3/akka-http-spray-json_2.13-10.5.3.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/circe/circe-core_2.13/0.14.7/circe-core_2.13-0.14.7.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/circe/circe-generic_2.13/0.14.7/circe-generic_2.13-0.14.7.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/circe/circe-parser_2.13/0.14.7/circe-parser_2.13-0.14.7.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/ch/qos/logback/logback-classic/1.5.6/logback-classic-1.5.6.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/scala-logging/scala-logging_2.13/3.9.5/scala-logging_2.13-3.9.5.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-stream-kafka_2.13/4.0.2/akka-stream-kafka_2.13-4.0.2.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.5.1/kafka-clients-3.5.1.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/github/Ma27/rediscala_2.13/1.9.1/rediscala_2.13-1.9.1.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scalameta/munit_2.13/0.7.29/munit_2.13-0.7.29.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-actor_2.13/2.8.5/akka-actor_2.13-2.8.5.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-protobuf-v3_2.13/2.8.5/akka-protobuf-v3_2.13-2.8.5.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.4/reactive-streams-1.0.4.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/ssl-config-core_2.13/0.6.1/ssl-config-core_2.13-0.6.1.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-http-core_2.13/10.5.3/akka-http-core_2.13-10.5.3.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/spray/spray-json_2.13/1.3.6/spray-json_2.13-1.3.6.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/circe/circe-numbers_2.13/0.14.7/circe-numbers_2.13-0.14.7.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/typelevel/cats-core_2.13/2.10.0/cats-core_2.13-2.10.0.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/chuusai/shapeless_2.13/2.3.10/shapeless_2.13-2.3.10.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/circe/circe-jawn_2.13/0.14.7/circe-jawn_2.13-0.14.7.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/ch/qos/logback/logback-core/1.5.6/logback-core-1.5.6.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.13/slf4j-api-2.0.13.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.13.16/scala-reflect-2.13.16.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/github/luben/zstd-jni/1.5.5-1/zstd-jni-1.5.5-1.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.1/snappy-java-1.1.10.1.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-stm/scala-stm_2.13/0.9.1/scala-stm_2.13-0.9.1.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scalameta/junit-interface/0.7.29/junit-interface-0.7.29.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/junit/junit/4.13.2/junit-4.13.2.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/config/1.4.2/config-1.4.2.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-java8-compat_2.13/1.0.0/scala-java8-compat_2.13-1.0.0.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-parsing_2.13/10.5.3/akka-parsing_2.13-10.5.3.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/typelevel/cats-kernel_2.13/2.10.0/cats-kernel_2.13-2.10.0.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/typelevel/jawn-parser_2.13/1.5.1/jawn-parser_2.13-1.5.1.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar -Xplugin-require:semanticdb -Yrangepos -Ymacro-expand:discard -Ycache-plugin-class-loader:last-modified -Ypresentation-any-thread

  last tree to typer: Apply(method +)
       tree position: line 390 of file://<WORKSPACE>/src/test/scala/gibbon/performance/CheckpointPerformanceSpec.scala
            tree tpe: String
              symbol: final method + in class String
   symbol definition: final def +(x$1: Any): String (a MethodSymbol)
      symbol package: java.lang
       symbol owners: method + -> class String
           call site: <none> in <none>

== Source file context for tree position ==

   387       assertEquals(retrievedCount, concurrentOperations * operationCount)
   388       
   389       // Should achieve reasonable performance
   390       assert(_CURSOR_opsPerSec > 500.0, s"EventStore operations too slow: $opsPerSec ops/sec")
   391       
   392       // Final state should reflect deletions (every 10th item deleted)
   393       val expectedFinalSize = concurrentOperations * operationCount * 0.9 // 90% remaining

occurred in the presentation compiler.



action parameters:
offset: 14262
uri: file://<WORKSPACE>/src/test/scala/gibbon/performance/CheckpointPerformanceSpec.scala
text:
```scala
package gibbon.performance

import gibbon.checkpoint.{CheckpointManager, Checkpoint, CheckpointMetadata, CheckpointType, Periodic}
import gibbon.flows.CheckpointingFlow
import gibbon.operations.EventStore
import gibbon.core.Event
import munit.FunSuite
import akka.actor.ActorSystem
import akka.stream.scaladsl.{Source => AkkaSource, Sink => AkkaSink, Flow => AkkaFlow}
import scala.concurrent.{Future, ExecutionContext, Promise}
import scala.concurrent.duration._
import java.time.Instant
import java.util.concurrent.atomic.{AtomicReference, AtomicLong, AtomicInteger}
import scala.util.{Success, Failure, Random}
import io.circe._
import io.circe.syntax._
import io.circe.parser._
import io.circe.generic.auto._

/**
 * Performance test suite for checkpointing under load
 * Tests system behavior with high throughput, concurrent operations, and stress conditions
 */
class CheckpointPerformanceSpec extends FunSuite {
  
  implicit val system: ActorSystem = ActorSystem("CheckpointPerformanceSpec")
  implicit val ec: ExecutionContext = system.dispatcher
  
  override def afterAll(): Unit = {
    system.terminate()
    super.afterAll()
  }
  
  // Test data types
  case class PerformanceEvent(id: Long, data: String, timestamp: Long = System.currentTimeMillis())
  case class ProcessingMetrics(
    totalProcessed: Long,
    checkpointsCreated: Long,
    averageProcessingTime: Double,
    maxProcessingTime: Long,
    minProcessingTime: Long
  )
  
  // High-performance CheckpointManager implementation for testing
  class PerformanceCheckpointManager extends CheckpointManager[String, PerformanceEvent] {
    private val checkpoints = new AtomicReference(Map.empty[String, Checkpoint[String, PerformanceEvent]])
    private val saveCount = new AtomicLong(0)
    private val loadCount = new AtomicLong(0)
    private val saveLatencies = new AtomicReference(List.empty[Long])
    private val loadLatencies = new AtomicReference(List.empty[Long])
    
    override def saveCheckpoint(checkpoint: Checkpoint[String, PerformanceEvent]): Future[Unit] = {
      val startTime = System.nanoTime()
      
      checkpoints.updateAndGet(_ + (checkpoint.pipelineId -> checkpoint))
      saveCount.incrementAndGet()
      
      val endTime = System.nanoTime()
      val latency = endTime - startTime
      saveLatencies.updateAndGet(latency :: _)
      
      // Simulate some processing delay
      if (Random.nextDouble() < 0.1) { // 10% chance of slower save
        Thread.sleep(1)
      }
      
      Future.successful(())
    }
    
    override def loadCheckpoint(pipelineId: String): Future[Option[Checkpoint[String, PerformanceEvent]]] = {
      val startTime = System.nanoTime()
      
      val result = checkpoints.get().get(pipelineId)
      loadCount.incrementAndGet()
      
      val endTime = System.nanoTime()
      val latency = endTime - startTime
      loadLatencies.updateAndGet(latency :: _)
      
      Future.successful(result)
    }
    
    override def listCheckpoints(pipelineId: String): Future[List[CheckpointMetadata]] = {
      val checkpoint = checkpoints.get().get(pipelineId)
      val metadata = checkpoint.map { cp =>
        CheckpointMetadata(
          pipelineId = cp.pipelineId,
          version = cp.offset,
          createdAt = cp.timestamp,
          checkpointType = Periodic
        )
      }.toList
      Future.successful(metadata)
    }
    
    override def deleteCheckpoint(pipelineId: String, version: Long): Future[Unit] = {
      checkpoints.updateAndGet(_ - pipelineId)
      Future.successful(())
    }
    
    // Performance metrics methods
    def getSaveCount: Long = saveCount.get()
    def getLoadCount: Long = loadCount.get()
    def getCheckpointCount: Int = checkpoints.get().size
    
    def getSaveLatencyStats: (Double, Long, Long) = {
      val latencies = saveLatencies.get()
      if (latencies.isEmpty) (0.0, 0L, 0L)
      else {
        val avg = latencies.sum.toDouble / latencies.length
        val max = latencies.max
        val min = latencies.min
        (avg, max, min)
      }
    }
    
    def getLoadLatencyStats: (Double, Long, Long) = {
      val latencies = loadLatencies.get()
      if (latencies.isEmpty) (0.0, 0L, 0L)
      else {
        val avg = latencies.sum.toDouble / latencies.length
        val max = latencies.max
        val min = latencies.min
        (avg, max, min)
      }
    }
    
    def clear(): Unit = {
      checkpoints.set(Map.empty)
      saveCount.set(0)
      loadCount.set(0)
      saveLatencies.set(List.empty)
      loadLatencies.set(List.empty)
    }
  }
  
  // High-performance EventStore implementation for testing
  class PerformanceEventStore extends EventStore[String, PerformanceEvent] {
    private val storage = new AtomicReference(Map.empty[String, PerformanceEvent])
    private val operationCount = new AtomicLong(0)
    private val operationLatencies = new AtomicReference(List.empty[Long])
    
    override def get(key: String): Future[Option[PerformanceEvent]] = {
      val startTime = System.nanoTime()
      val result = storage.get().get(key)
      recordOperation(startTime)
      Future.successful(result)
    }
    
    override def put(key: String, value: PerformanceEvent): Future[Unit] = {
      val startTime = System.nanoTime()
      storage.updateAndGet(_ + (key -> value))
      recordOperation(startTime)
      Future.successful(())
    }
    
    override def delete(key: String): Future[Unit] = {
      val startTime = System.nanoTime()
      storage.updateAndGet(_ - key)
      recordOperation(startTime)
      Future.successful(())
    }
    
    override def getAll: Future[Map[String, PerformanceEvent]] = {
      val startTime = System.nanoTime()
      val result = storage.get()
      recordOperation(startTime)
      Future.successful(result)
    }
    
    private def recordOperation(startTime: Long): Unit = {
      operationCount.incrementAndGet()
      val endTime = System.nanoTime()
      val latency = endTime - startTime
      operationLatencies.updateAndGet(latency :: _)
    }
    
    // Performance metrics methods
    def getOperationCount: Long = operationCount.get()
    def getStorageSize: Int = storage.get().size
    
    def getOperationLatencyStats: (Double, Long, Long) = {
      val latencies = operationLatencies.get()
      if (latencies.isEmpty) (0.0, 0L, 0L)
      else {
        val avg = latencies.sum.toDouble / latencies.length
        val max = latencies.max
        val min = latencies.min
        (avg, max, min)
      }
    }
    
    def clear(): Unit = {
      storage.set(Map.empty)
      operationCount.set(0)
      operationLatencies.set(List.empty)
    }
  }
  
  // Performance monitoring utilities
  class PerformanceMonitor {
    private val startTime = new AtomicReference(System.currentTimeMillis())
    private val processedCount = new AtomicLong(0)
    private val processingTimes = new AtomicReference(List.empty[Long])
    
    def recordProcessing(processingTime: Long): Unit = {
      processedCount.incrementAndGet()
      processingTimes.updateAndGet(processingTime :: _)
    }
    
    def getMetrics: ProcessingMetrics = {
      val times = processingTimes.get()
      val totalProcessed = processedCount.get()
      
      if (times.isEmpty) {
        ProcessingMetrics(totalProcessed, 0, 0.0, 0L, 0L)
      } else {
        val avg = times.sum.toDouble / times.length
        val max = times.max
        val min = times.min
        ProcessingMetrics(totalProcessed, 0, avg, max, min)
      }
    }
    
    def getThroughput: Double = {
      val elapsed = System.currentTimeMillis() - startTime.get()
      val processed = processedCount.get()
      if (elapsed > 0) processed.toDouble / (elapsed / 1000.0) else 0.0
    }
    
    def reset(): Unit = {
      startTime.set(System.currentTimeMillis())
      processedCount.set(0)
      processingTimes.set(List.empty)
    }
  }
  
  test("CheckpointingFlow should handle high-throughput event processing") {
    val checkpointManager = new PerformanceCheckpointManager()
    val monitor = new PerformanceMonitor()
    
    val eventCount = 10000
    val events = (1 to eventCount).map(i => PerformanceEvent(i, s"high-throughput-event-$i"))
    
    val checkpointingFlow = new CheckpointingFlow[PerformanceEvent, String, PerformanceEvent](
      pipelineId = "high-throughput-test",
      checkpointInterval = 100.millis, // Frequent checkpoints
      extractOffset = _.id,
      checkpointManager = checkpointManager
    )
    
    val result = for {
      startTime <- Future.successful(System.currentTimeMillis())
      
      processedEvents <- AkkaSource(events)
        .via(checkpointingFlow.toAkkaFlow())
        .map { event =>
          val processingTime = System.nanoTime()
          monitor.recordProcessing(processingTime)
          event
        }
        .runWith(AkkaSink.seq)
      
      endTime <- Future.successful(System.currentTimeMillis())
      
      // Wait for checkpoints to be saved
      _ <- akka.pattern.after(500.millis, system.scheduler)(Future.successful(()))
      
    } yield {
      val elapsedTime = endTime - startTime
      val throughput = eventCount.toDouble / (elapsedTime / 1000.0)
      (processedEvents.length, throughput, checkpointManager.getSaveCount, monitor.getMetrics)
    }
    
    result.map { case (processedCount, throughput, checkpointCount, metrics) =>
      // All events should be processed
      assertEquals(processedCount, eventCount)
      
      // Should achieve reasonable throughput (adjust based on system capabilities)
      assert(throughput > 100.0, s"Throughput too low: $throughput events/sec")
      
      // Should create multiple checkpoints
      assert(checkpointCount > 0, s"No checkpoints created: $checkpointCount")
      
      // Processing metrics should be reasonable
      assert(metrics.totalProcessed == eventCount)
      
      println(s"High-throughput test: $throughput events/sec, $checkpointCount checkpoints")
    }
  }
  
  test("CheckpointManager should handle concurrent checkpoint operations") {
    val checkpointManager = new PerformanceCheckpointManager()
    val concurrentPipelines = 50
    val checkpointsPerPipeline = 20
    
    val concurrentOperations = (1 to concurrentPipelines).map { pipelineId =>
      val pipelineName = s"concurrent-pipeline-$pipelineId"
      
      // Create multiple checkpoints for each pipeline concurrently
      val checkpointOperations = (1 to checkpointsPerPipeline).map { checkpointId =>
        val checkpoint = Checkpoint[String, PerformanceEvent](
          pipelineId = pipelineName,
          offset = checkpointId.toLong,
          timestamp = Instant.now(),
          lastProcessedEvent = Some(Event(s"key-$checkpointId", PerformanceEvent(checkpointId, s"data-$checkpointId"), System.currentTimeMillis())),
          state = Map("processedCount" -> checkpointId.toString)
        )
        checkpointManager.saveCheckpoint(checkpoint)
      }
      
      Future.sequence(checkpointOperations)
    }
    
    val result = for {
      startTime <- Future.successful(System.currentTimeMillis())
      _ <- Future.sequence(concurrentOperations)
      endTime <- Future.successful(System.currentTimeMillis())
      
      // Verify all checkpoints were saved
      loadOperations = (1 to concurrentPipelines).map { pipelineId =>
        checkpointManager.loadCheckpoint(s"concurrent-pipeline-$pipelineId")
      }
      loadedCheckpoints <- Future.sequence(loadOperations)
      
    } yield {
      val elapsedTime = endTime - startTime
      val totalOperations = concurrentPipelines * checkpointsPerPipeline
      val operationsPerSecond = totalOperations.toDouble / (elapsedTime / 1000.0)
      (loadedCheckpoints, operationsPerSecond, checkpointManager.getSaveLatencyStats)
    }
    
    result.map { case (loadedCheckpoints, opsPerSec, (avgLatency, maxLatency, minLatency)) =>
      // All pipelines should have their latest checkpoint
      assertEquals(loadedCheckpoints.length, concurrentPipelines)
      loadedCheckpoints.foreach { checkpointOpt =>
        assert(checkpointOpt.isDefined)
        assertEquals(checkpointOpt.get.offset, checkpointsPerPipeline.toLong) // Latest checkpoint
      }
      
      // Should achieve reasonable concurrent performance
      assert(opsPerSec > 100.0, s"Concurrent operations too slow: $opsPerSec ops/sec")
      
      // Latency should be reasonable (in nanoseconds)
      assert(avgLatency < 10000000, s"Average latency too high: ${avgLatency / 1000000.0} ms") // < 10ms
      
      println(s"Concurrent test: $opsPerSec ops/sec, avg latency: ${avgLatency / 1000000.0} ms")
    }
  }
  
  test("EventStore should maintain performance under heavy load") {
    val eventStore = new PerformanceEventStore()
    val operationCount = 5000
    val concurrentOperations = 10
    
    val heavyLoadOperations = (1 to concurrentOperations).map { threadId =>
      val operations = (1 to operationCount).map { opId =>
        val key = s"thread-$threadId-key-$opId"
        val event = PerformanceEvent(opId, s"thread-$threadId-data-$opId")
        
        for {
          _ <- eventStore.put(key, event)
          retrieved <- eventStore.get(key)
          _ <- if (opId % 10 == 0) eventStore.delete(key) else Future.successful(())
        } yield retrieved
      }
      
      Future.sequence(operations)
    }
    
    val result = for {
      startTime <- Future.successful(System.currentTimeMillis())
      results <- Future.sequence(heavyLoadOperations)
      endTime <- Future.successful(System.currentTimeMillis())
      
      finalState <- eventStore.getAll
      
    } yield {
      val elapsedTime = endTime - startTime
      val totalOps = concurrentOperations * operationCount * 2 // put + get operations
      val opsPerSecond = totalOps.toDouble / (elapsedTime / 1000.0)
      (results.flatten.length, opsPerSecond, finalState.size, eventStore.getOperationLatencyStats)
    }
    
    result.map { case (retrievedCount, opsPerSec, finalStateSize, (avgLatency, maxLatency, minLatency)) =>
      // Should have processed all operations
      assertEquals(retrievedCount, concurrentOperations * operationCount)
      
      // Should achieve reasonable performance
      assert(@@opsPerSec > 500.0, s"EventStore operations too slow: $opsPerSec ops/sec")
      
      // Final state should reflect deletions (every 10th item deleted)
      val expectedFinalSize = concurrentOperations * operationCount * 0.9 // 90% remaining
      assert(finalStateSize >= expectedFinalSize * 0.8, s"Unexpected final state size: $finalStateSize")
      
      // Latency should be reasonable
      assert(avgLatency < 5000000, s"Average latency too high: ${avgLatency / 1000000.0} ms") // < 5ms
      
      println(s"Heavy load test: $opsPerSec ops/sec, final state: $finalStateSize items")
    }
  }
  
  test("System should handle checkpoint recovery under load") {
    val checkpointManager = new PerformanceCheckpointManager()
    val eventStore = new PerformanceEventStore()
    val monitor = new PerformanceMonitor()
    
    val pipelineCount = 20
    val eventsPerPipeline = 1000
    
    // Create initial state and checkpoints for multiple pipelines
    val setupOperations = (1 to pipelineCount).map { pipelineId =>
      val pipelineName = s"recovery-pipeline-$pipelineId"
      
      // Store some state in EventStore
      val stateOperations = (1 to 10).map { stateId =>
        val key = s"$pipelineName-state-$stateId"
        val event = PerformanceEvent(stateId, s"state-data-$stateId")
        eventStore.put(key, event)
      }
      
      // Create checkpoint
      val checkpoint = Checkpoint[String, PerformanceEvent](
        pipelineId = pipelineName,
        offset = (eventsPerPipeline / 2).toLong, // Checkpoint at halfway point
        timestamp = Instant.now(),
        lastProcessedEvent = Some(Event(s"key-${eventsPerPipeline/2}", PerformanceEvent(eventsPerPipeline/2, s"data-${eventsPerPipeline/2}"), System.currentTimeMillis())),
        state = Map(
          "processedCount" -> (eventsPerPipeline / 2).toString,
          "status" -> "checkpointed"
        )
      )
      
      for {
        _ <- Future.sequence(stateOperations)
        _ <- checkpointManager.saveCheckpoint(checkpoint)
      } yield pipelineName
    }
    
    val result = for {
      pipelineNames <- Future.sequence(setupOperations)
      
      // Simulate recovery for all pipelines concurrently
      startTime <- Future.successful(System.currentTimeMillis())
      
      recoveryOperations = pipelineNames.map { pipelineName =>
        for {
          checkpoint <- checkpointManager.loadCheckpoint(pipelineName)
          states <- eventStore.getAll
          _ <- Future.successful(monitor.recordProcessing(System.nanoTime()))
        } yield (checkpoint, states.size)
      }
      
      recoveryResults <- Future.sequence(recoveryOperations)
      endTime <- Future.successful(System.currentTimeMillis())
      
    } yield {
      val elapsedTime = endTime - startTime
      val recoveryThroughput = pipelineCount.toDouble / (elapsedTime / 1000.0)
      (recoveryResults, recoveryThroughput, monitor.getMetrics)
    }
    
    result.map { case (recoveryResults, throughput, metrics) =>
      // All pipelines should recover successfully
      assertEquals(recoveryResults.length, pipelineCount)
      
      recoveryResults.foreach { case (checkpointOpt, stateSize) =>
        assert(checkpointOpt.isDefined)
        assertEquals(checkpointOpt.get.offset, (eventsPerPipeline / 2).toLong)
        assertEquals(checkpointOpt.get.state("processedCount"), (eventsPerPipeline / 2).toString)
        
        // State should be available
        assert(stateSize >= 10) // At least 10 state items per pipeline
      }
      
      // Recovery should be fast
      assert(throughput > 10.0, s"Recovery too slow: $throughput pipelines/sec")
      
      println(s"Recovery test: $throughput pipelines/sec, ${metrics.totalProcessed} operations")
    }
  }
  
  test("Memory usage should remain stable under continuous checkpointing") {
    val checkpointManager = new PerformanceCheckpointManager()
    val eventStore = new PerformanceEventStore()
    
    val runtime = Runtime.getRuntime
    val initialMemory = runtime.totalMemory() - runtime.freeMemory()
    
    val continuousOperations = (1 to 100).map { batchId =>
      val batchOperations = (1 to 100).map { opId =>
        val pipelineId = s"memory-test-${batchId % 10}" // Reuse pipeline IDs
        val key = s"batch-$batchId-op-$opId"
        val event = PerformanceEvent(opId, s"batch-$batchId-data-$opId")
        
        val checkpoint = Checkpoint[String, PerformanceEvent](
          pipelineId = pipelineId,
          offset = (batchId * 100 + opId).toLong,
          timestamp = Instant.now(),
          lastProcessedEvent = Some(Event(key, event, System.currentTimeMillis())),
          state = Map("batchId" -> batchId.toString, "opId" -> opId.toString)
        )
        
        for {
          _ <- eventStore.put(key, event)
          _ <- checkpointManager.saveCheckpoint(checkpoint)
          // Occasionally clean up old data
          _ <- if (opId % 50 == 0) eventStore.delete(s"batch-${batchId-5}-op-$opId") else Future.successful(())
        } yield ()
      }
      
      Future.sequence(batchOperations).map { _ =>
        // Force garbage collection periodically
        if (batchId % 20 == 0) {
          System.gc()
          Thread.sleep(10)
        }
        
        val currentMemory = runtime.totalMemory() - runtime.freeMemory()
        val memoryIncrease = currentMemory - initialMemory
        (batchId, memoryIncrease)
      }
    }
    
    val result = for {
      memorySnapshots <- Future.sequence(continuousOperations)
      finalMemory <- Future.successful(runtime.totalMemory() - runtime.freeMemory())
      
      // Get final metrics
      finalCheckpointCount = checkpointManager.getCheckpointCount
      finalEventStoreSize = eventStore.getStorageSize
      
    } yield {
      val totalMemoryIncrease = finalMemory - initialMemory
      val maxMemoryIncrease = memorySnapshots.map(_._2).max
      (totalMemoryIncrease, maxMemoryIncrease, finalCheckpointCount, finalEventStoreSize)
    }
    
    result.map { case (totalIncrease, maxIncrease, checkpointCount, eventStoreSize) =>
      // Memory increase should be reasonable (less than 100MB for this test)
      val maxAllowedIncrease = 100 * 1024 * 1024 // 100MB
      assert(totalIncrease < maxAllowedIncrease, 
        s"Memory increase too high: ${totalIncrease / (1024 * 1024)} MB")
      
      assert(maxIncrease < maxAllowedIncrease * 1.5, 
        s"Peak memory increase too high: ${maxIncrease / (1024 * 1024)} MB")
      
      // Should have reasonable final state sizes
      assert(checkpointCount <= 10, s"Too many checkpoints retained: $checkpointCount")
      assert(eventStoreSize < 10000, s"EventStore too large: $eventStoreSize items")
      
      println(s"Memory test: ${totalIncrease / (1024 * 1024)} MB increase, $checkpointCount checkpoints, $eventStoreSize events")
    }
  }
  
  test("System should handle checkpoint failures gracefully under load") {
    val checkpointManager = new PerformanceCheckpointManager() {
      private val failureRate = 0.1 // 10% failure rate
      
      override def saveCheckpoint(checkpoint: Checkpoint[String, PerformanceEvent]): Future[Unit] = {
        if (Random.nextDouble() < failureRate) {
          Future.failed(new RuntimeException("Simulated checkpoint failure"))
        } else {
          super.saveCheckpoint(checkpoint)
        }
      }
    }
    
    val eventCount = 1000
    val events = (1 to eventCount).map(i => PerformanceEvent(i, s"failure-test-event-$i"))
    
    val checkpointingFlow = new CheckpointingFlow[PerformanceEvent, String, PerformanceEvent](
      pipelineId = "failure-test",
      checkpointInterval = 50.millis, // Frequent checkpoints to trigger failures
      extractOffset = _.id,
      checkpointManager = checkpointManager
    )
    
    val result = for {
      processedEvents <- AkkaSource(events)
        .via(checkpointingFlow.toAkkaFlow())
        .runWith(AkkaSink.seq)
      
      // Wait for checkpoint operations to complete
      _ <- akka.pattern.after(1.second, system.scheduler)(Future.successful(()))
      
      finalCheckpoint <- checkpointManager.loadCheckpoint("failure-test")
      
    } yield (processedEvents.length, checkpointManager.getSaveCount, finalCheckpoint)
    
    result.map { case (processedCount, saveAttempts, finalCheckpoint) =>
      // All events should still be processed despite checkpoint failures
      assertEquals(processedCount, eventCount)
      
      // Should have attempted multiple saves
      assert(saveAttempts > 0, "No checkpoint save attempts")
      
      // May or may not have a final checkpoint due to failures, but system should continue
      println(s"Failure test: $processedCount events processed, $saveAttempts save attempts, final checkpoint: ${finalCheckpoint.isDefined}")
    }
  }
}
```


presentation compiler configuration:
Scala version: 2.13.16
Classpath:
<WORKSPACE>/.bloop/root/bloop-bsp-clients-classes/test-classes-Metals-Ve0ZHR_bS9ClHmab5hD_vg== [exists ], <HOME>/Library/Caches/bloop/semanticdb/com.sourcegraph.semanticdb-javac.0.11.0/semanticdb-javac-0.11.0.jar [exists ], <WORKSPACE>/.bloop/root/bloop-bsp-clients-classes/classes-Metals-Ve0ZHR_bS9ClHmab5hD_vg== [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.13.16/scala-library-2.13.16.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-stream_2.13/2.8.5/akka-stream_2.13-2.8.5.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-http_2.13/10.5.3/akka-http_2.13-10.5.3.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-http-spray-json_2.13/10.5.3/akka-http-spray-json_2.13-10.5.3.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/circe/circe-core_2.13/0.14.7/circe-core_2.13-0.14.7.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/circe/circe-generic_2.13/0.14.7/circe-generic_2.13-0.14.7.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/circe/circe-parser_2.13/0.14.7/circe-parser_2.13-0.14.7.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/ch/qos/logback/logback-classic/1.5.6/logback-classic-1.5.6.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/scala-logging/scala-logging_2.13/3.9.5/scala-logging_2.13-3.9.5.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-stream-kafka_2.13/4.0.2/akka-stream-kafka_2.13-4.0.2.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.5.1/kafka-clients-3.5.1.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/github/Ma27/rediscala_2.13/1.9.1/rediscala_2.13-1.9.1.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scalameta/munit_2.13/0.7.29/munit_2.13-0.7.29.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-actor_2.13/2.8.5/akka-actor_2.13-2.8.5.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-protobuf-v3_2.13/2.8.5/akka-protobuf-v3_2.13-2.8.5.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.4/reactive-streams-1.0.4.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/ssl-config-core_2.13/0.6.1/ssl-config-core_2.13-0.6.1.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-http-core_2.13/10.5.3/akka-http-core_2.13-10.5.3.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/spray/spray-json_2.13/1.3.6/spray-json_2.13-1.3.6.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/circe/circe-numbers_2.13/0.14.7/circe-numbers_2.13-0.14.7.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/typelevel/cats-core_2.13/2.10.0/cats-core_2.13-2.10.0.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/chuusai/shapeless_2.13/2.3.10/shapeless_2.13-2.3.10.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/circe/circe-jawn_2.13/0.14.7/circe-jawn_2.13-0.14.7.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/ch/qos/logback/logback-core/1.5.6/logback-core-1.5.6.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.13/slf4j-api-2.0.13.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.13.16/scala-reflect-2.13.16.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/github/luben/zstd-jni/1.5.5-1/zstd-jni-1.5.5-1.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.1/snappy-java-1.1.10.1.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-stm/scala-stm_2.13/0.9.1/scala-stm_2.13-0.9.1.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scalameta/junit-interface/0.7.29/junit-interface-0.7.29.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/junit/junit/4.13.2/junit-4.13.2.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/config/1.4.2/config-1.4.2.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-java8-compat_2.13/1.0.0/scala-java8-compat_2.13-1.0.0.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-parsing_2.13/10.5.3/akka-parsing_2.13-10.5.3.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/typelevel/cats-kernel_2.13/2.10.0/cats-kernel_2.13-2.10.0.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/typelevel/jawn-parser_2.13/1.5.1/jawn-parser_2.13-1.5.1.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar [exists ], <HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar [exists ]
Options:
-Yrangepos -Xplugin-require:semanticdb




#### Error stacktrace:

```
scala.reflect.internal.Reporting.abort(Reporting.scala:70)
	scala.reflect.internal.Reporting.abort$(Reporting.scala:66)
	scala.reflect.internal.SymbolTable.abort(SymbolTable.scala:28)
	scala.reflect.internal.Types$ThisType.<init>(Types.scala:1394)
	scala.reflect.internal.Types$UniqueThisType.<init>(Types.scala:1414)
	scala.reflect.internal.Types$ThisType$.apply(Types.scala:1418)
	scala.meta.internal.pc.AutoImportsProvider$$anonfun$1.applyOrElse(AutoImportsProvider.scala:94)
	scala.meta.internal.pc.AutoImportsProvider$$anonfun$1.applyOrElse(AutoImportsProvider.scala:79)
	scala.collection.immutable.List.collect(List.scala:268)
	scala.meta.internal.pc.AutoImportsProvider.autoImports(AutoImportsProvider.scala:79)
	scala.meta.internal.pc.ScalaPresentationCompiler.$anonfun$autoImports$1(ScalaPresentationCompiler.scala:404)
	scala.meta.internal.pc.CompilerAccess.retryWithCleanCompiler(CompilerAccess.scala:182)
	scala.meta.internal.pc.CompilerAccess.$anonfun$withSharedCompiler$1(CompilerAccess.scala:155)
	scala.Option.map(Option.scala:242)
	scala.meta.internal.pc.CompilerAccess.withSharedCompiler(CompilerAccess.scala:154)
	scala.meta.internal.pc.CompilerAccess.$anonfun$withInterruptableCompiler$1(CompilerAccess.scala:92)
	scala.meta.internal.pc.CompilerAccess.$anonfun$onCompilerJobQueue$1(CompilerAccess.scala:209)
	scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:152)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1095)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:619)
	java.base/java.lang.Thread.run(Thread.java:1447)
```
#### Short summary: 

scala.reflect.internal.FatalError: 
  ThisType(value $anonfun) for sym which is not a class
     while compiling: file://<WORKSPACE>/src/test/scala/gibbon/performance/CheckpointPerformanceSpec.scala
        during phase: globalPhase=<no phase>, enteringPhase=parser
     library version: version 2.13.16
    compiler version: version 2.13.16
  reconstructed args: -classpath <WORKSPACE>/.bloop/root/bloop-bsp-clients-classes/test-classes-Metals-Ve0ZHR_bS9ClHmab5hD_vg==:<HOME>/Library/Caches/bloop/semanticdb/com.sourcegraph.semanticdb-javac.0.11.0/semanticdb-javac-0.11.0.jar:<WORKSPACE>/.bloop/root/bloop-bsp-clients-classes/classes-Metals-Ve0ZHR_bS9ClHmab5hD_vg==:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.13.16/scala-library-2.13.16.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-stream_2.13/2.8.5/akka-stream_2.13-2.8.5.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-http_2.13/10.5.3/akka-http_2.13-10.5.3.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-http-spray-json_2.13/10.5.3/akka-http-spray-json_2.13-10.5.3.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/circe/circe-core_2.13/0.14.7/circe-core_2.13-0.14.7.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/circe/circe-generic_2.13/0.14.7/circe-generic_2.13-0.14.7.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/circe/circe-parser_2.13/0.14.7/circe-parser_2.13-0.14.7.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/ch/qos/logback/logback-classic/1.5.6/logback-classic-1.5.6.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/scala-logging/scala-logging_2.13/3.9.5/scala-logging_2.13-3.9.5.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-stream-kafka_2.13/4.0.2/akka-stream-kafka_2.13-4.0.2.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.5.1/kafka-clients-3.5.1.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/github/Ma27/rediscala_2.13/1.9.1/rediscala_2.13-1.9.1.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scalameta/munit_2.13/0.7.29/munit_2.13-0.7.29.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-actor_2.13/2.8.5/akka-actor_2.13-2.8.5.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-protobuf-v3_2.13/2.8.5/akka-protobuf-v3_2.13-2.8.5.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.4/reactive-streams-1.0.4.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/ssl-config-core_2.13/0.6.1/ssl-config-core_2.13-0.6.1.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-http-core_2.13/10.5.3/akka-http-core_2.13-10.5.3.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/spray/spray-json_2.13/1.3.6/spray-json_2.13-1.3.6.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/circe/circe-numbers_2.13/0.14.7/circe-numbers_2.13-0.14.7.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/typelevel/cats-core_2.13/2.10.0/cats-core_2.13-2.10.0.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/chuusai/shapeless_2.13/2.3.10/shapeless_2.13-2.3.10.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/circe/circe-jawn_2.13/0.14.7/circe-jawn_2.13-0.14.7.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/ch/qos/logback/logback-core/1.5.6/logback-core-1.5.6.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.13/slf4j-api-2.0.13.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.13.16/scala-reflect-2.13.16.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/github/luben/zstd-jni/1.5.5-1/zstd-jni-1.5.5-1.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.1/snappy-java-1.1.10.1.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-stm/scala-stm_2.13/0.9.1/scala-stm_2.13-0.9.1.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scalameta/junit-interface/0.7.29/junit-interface-0.7.29.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/junit/junit/4.13.2/junit-4.13.2.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/config/1.4.2/config-1.4.2.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-java8-compat_2.13/1.0.0/scala-java8-compat_2.13-1.0.0.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/typesafe/akka/akka-parsing_2.13/10.5.3/akka-parsing_2.13-10.5.3.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/typelevel/cats-kernel_2.13/2.10.0/cats-kernel_2.13-2.10.0.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/typelevel/jawn-parser_2.13/1.5.1/jawn-parser_2.13-1.5.1.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:<HOME>/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar -Xplugin-require:semanticdb -Yrangepos -Ymacro-expand:discard -Ycache-plugin-class-loader:last-modified -Ypresentation-any-thread

  last tree to typer: Apply(method +)
       tree position: line 390 of file://<WORKSPACE>/src/test/scala/gibbon/performance/CheckpointPerformanceSpec.scala
            tree tpe: String
              symbol: final method + in class String
   symbol definition: final def +(x$1: Any): String (a MethodSymbol)
      symbol package: java.lang
       symbol owners: method + -> class String
           call site: <none> in <none>

== Source file context for tree position ==

   387       assertEquals(retrievedCount, concurrentOperations * operationCount)
   388       
   389       // Should achieve reasonable performance
   390       assert(_CURSOR_opsPerSec > 500.0, s"EventStore operations too slow: $opsPerSec ops/sec")
   391       
   392       // Final state should reflect deletions (every 10th item deleted)
   393       val expectedFinalSize = concurrentOperations * operationCount * 0.9 // 90% remaining